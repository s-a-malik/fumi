{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "clip.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3lt1TRWsBu4"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torch.utils.data as data\n",
        "import json\n",
        "import os\n",
        "from PIL import Image\n",
        "import subprocess\n",
        "from typing import List\n",
        "from torchvision.transforms import Compose\n",
        "from torchvision import transforms\n",
        "import torch.optim as optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pf312KuUvS7p",
        "outputId": "a8f862ac-24dc-4686-d843-1d25a33bbf22"
      },
      "source": [
        "device = torch.device('cuda:' + str(0) if torch.cuda.is_available() else 'cpu')\n",
        "#device = torch.device('cpu')\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSsEsvNbvCLr"
      },
      "source": [
        "# Create Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcyWODY9Oq_2"
      },
      "source": [
        "def generate_fake_embeddings(n=2, d=5):\n",
        "  base_truth = np.random.normal(size=(n, d))\n",
        "\n",
        "  emb1 = base_truth ** 2\n",
        "  emb2 = np.concatenate((base_truth, np.exp(base_truth)), axis=1)\n",
        "\n",
        "  return emb1, emb2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlQEhrlNP5dr"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXR6DrOPP7fp"
      },
      "source": [
        "class CLIP(nn.Module):\n",
        "\n",
        "  def __init__(self, text_input_dim, image_input_dim, latent_dim):\n",
        "    super().__init__()\n",
        "\n",
        "    self.text_input_dim = text_input_dim\n",
        "    self.image_input_dim = image_input_dim\n",
        "    self.latent_dim = latent_dim\n",
        "\n",
        "    self.text_fc = nn.Linear(text_input_dim, latent_dim)\n",
        "    self.image_fc = nn.Linear(image_input_dim, latent_dim)\n",
        "\n",
        "  def forward(self, text, image):\n",
        "    #print(text.shape, image.shape, self.text_input_dim, self.image_input_dim)\n",
        "    # [batch_size, latent_dim]\n",
        "    text_latent = self.text_fc(text)\n",
        "    image_latent = self.image_fc(image)\n",
        "\n",
        "    text_norms = torch.linalg.norm(text_latent, axis=1)\n",
        "    image_norms = torch.linalg.norm(image_latent, axis=1)\n",
        "\n",
        "    text_norms_repeated = text_norms.repeat(len(text), 1).T\n",
        "    image_norms_repeated = image_norms.repeat(len(image), 1).T\n",
        "\n",
        "    cosine_sim_unnormalised = text_latent @ image_latent.T\n",
        "    #print(cosine_sim_unnormalised.shape, text_norms_repeated.shape, image_norms_repeated.shape)\n",
        "    cosine_sim_normalised = cosine_sim_unnormalised / text_norms_repeated / image_norms_repeated.T\n",
        "\n",
        "    return cosine_sim_normalised"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PjId7jVg1Cq"
      },
      "source": [
        "def train_clip(model, text_embeddings, image_embeddings, optimizer, loss_fn, batch_size, n_epochs):\n",
        "  assert text_embeddings.shape[0] == image_embeddings.shape[0]\n",
        "\n",
        "  xs_len = text_embeddings.shape[0]\n",
        "  n_batches = int(xs_len/batch_size)\n",
        "  \n",
        "  for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "\n",
        "    shuff_idxs = torch.randperm(xs_len)\n",
        "    for batch in range(n_batches):\n",
        "        batch_idxs = shuff_idxs[batch*batch_size:(batch+1)*batch_size]\n",
        "\n",
        "        batch_text = text_embeddings[batch_idxs]\n",
        "        batch_image = image_embeddings[batch_idxs]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(batch_text, batch_image)\n",
        "\n",
        "        # CHANGE\n",
        "        # ------------------------------------------CHANGE----------------------------------------------\n",
        "        # CHANGE\n",
        "        target = torch.eye(batch_size, batch_size).to(device)\n",
        "\n",
        "        #print(output.shape, target.shape)\n",
        "\n",
        "        loss = loss_fn(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLaXrEqJgyOP"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tewEKt0rg0Hp"
      },
      "source": [
        "text_embeddings, image_embeddings = generate_fake_embeddings(n=1024, d=16)\n",
        "\n",
        "text_embeddings = torch.Tensor(text_embeddings).to(device)\n",
        "image_embeddings = torch.Tensor(image_embeddings).to(device)\n",
        "\n",
        "model = CLIP(text_embeddings.shape[1], image_embeddings.shape[1], 12).to(device)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "loss_fn = nn.MSELoss().to(device)\n",
        "\n",
        "train_clip(model, text_embeddings, image_embeddings, optimizer, loss_fn, batch_size=64, n_epochs=5)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}