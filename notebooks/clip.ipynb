{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3lt1TRWsBu4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import json\n",
        "import os\n",
        "from PIL import Image\n",
        "import subprocess\n",
        "from typing import List\n",
        "import torch.optim as optim\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pf312KuUvS7p",
        "outputId": "692a6964-2c74-42ad-be1c-ce02fcaec0ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda:' + str(0) if torch.cuda.is_available() else 'cpu')\n",
        "#device = torch.device('cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSsEsvNbvCLr"
      },
      "source": [
        "# Create Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CcyWODY9Oq_2"
      },
      "outputs": [],
      "source": [
        "def generate_fake_embeddings(n_classes=32, n_images_per_class=128, dim=16):\n",
        "  text_emb = np.random.normal(size=(n_classes, dim))\n",
        "  diff = np.random.normal(size=(dim))\n",
        "\n",
        "  image_emb = np.random.normal(size=(n_classes, n_images_per_class, dim)) * 0.2\n",
        "  for c in range(n_classes):\n",
        "    for i in range(n_images_per_class):\n",
        "      for d in range(dim):\n",
        "        image_emb[c, i, d] = text_emb[c, d] * diff[d] + image_emb[c, i, d]\n",
        "  \n",
        "  image_emb = image_emb.reshape(n_classes * n_images_per_class, dim)\n",
        "  text_emb = np.repeat(text_emb, n_images_per_class, axis=0)\n",
        "  ids = np.repeat(np.arange(n_classes).astype(int), n_images_per_class, axis=0)\n",
        "\n",
        "  return torch.Tensor(text_emb).to(device), torch.Tensor(image_emb).to(device), ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JuNkKFfC9xZP"
      },
      "outputs": [],
      "source": [
        "text, img, ids = generate_fake_embeddings(4, 3, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bjs8HOOy97XQ",
        "outputId": "93331b12-ba07-491e-db7d-2a61653a794b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 1.6409, -1.0399],\n",
              "        [ 1.6409, -1.0399],\n",
              "        [ 1.6409, -1.0399],\n",
              "        [-0.3177,  0.1545],\n",
              "        [-0.3177,  0.1545],\n",
              "        [-0.3177,  0.1545],\n",
              "        [-0.4827,  0.4798],\n",
              "        [-0.4827,  0.4798],\n",
              "        [-0.4827,  0.4798],\n",
              "        [-0.4576, -0.3284],\n",
              "        [-0.4576, -0.3284],\n",
              "        [-0.4576, -0.3284]], device='cuda:0')"
            ]
          },
          "execution_count": 86,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VTSy-zy99WM",
        "outputId": "928f89e4-b04f-43be-adbf-c20127eff1f9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 0.0969,  0.4120],\n",
              "        [ 0.2877, -0.0457],\n",
              "        [ 0.0748,  0.0336],\n",
              "        [-1.4416,  1.3112],\n",
              "        [-1.8129,  1.0428],\n",
              "        [-1.3793,  1.3609],\n",
              "        [-1.8015,  1.5771],\n",
              "        [-1.7325,  1.4416],\n",
              "        [-1.7159,  1.3838],\n",
              "        [-1.8142,  0.7534],\n",
              "        [-1.8160,  0.9299],\n",
              "        [-1.4563,  0.7801]], device='cuda:0')"
            ]
          },
          "execution_count": 87,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_bWBHXu-Gp1",
        "outputId": "ec74726d-9c82-4191-c742-eccf968a5a98"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0., 0., 0., 1., 1., 1., 2., 2., 2., 3., 3., 3.], device='cuda:0')"
            ]
          },
          "execution_count": 99,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlQEhrlNP5dr"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXR6DrOPP7fp"
      },
      "outputs": [],
      "source": [
        "class CLIP(nn.Module):\n",
        "\n",
        "  def __init__(self, text_input_dim, image_input_dim, latent_dim):\n",
        "    super().__init__()\n",
        "\n",
        "    self.text_input_dim = text_input_dim\n",
        "    self.image_input_dim = image_input_dim\n",
        "    self.latent_dim = latent_dim\n",
        "\n",
        "    self.text_fc = nn.Linear(text_input_dim, latent_dim)\n",
        "    self.image_fc = nn.Linear(image_input_dim, latent_dim)\n",
        "\n",
        "  def forward(self, text, image):\n",
        "    #print(text.shape, image.shape, self.text_input_dim, self.image_input_dim)\n",
        "    # [batch_size, latent_dim]\n",
        "    text_latent = self.text_fc(text)\n",
        "    image_latent = self.image_fc(image)\n",
        "\n",
        "    text_norms = torch.linalg.norm(text_latent, axis=1)\n",
        "    image_norms = torch.linalg.norm(image_latent, axis=1)\n",
        "\n",
        "    text_norms_repeated = text_norms.repeat(len(image), 1).T\n",
        "    image_norms_repeated = image_norms.repeat(len(text), 1).T\n",
        "\n",
        "    cosine_sim_unnormalised = text_latent @ image_latent.T\n",
        "    #print(cosine_sim_unnormalised.shape, text_norms_repeated.shape, image_norms_repeated.shape)\n",
        "    cosine_sim_normalised = cosine_sim_unnormalised / text_norms_repeated / image_norms_repeated.T\n",
        "    #print(text_norms_repeated.shape, image_norms_repeated.shape, cosine_sim_normalised.shape)\n",
        "\n",
        "    return cosine_sim_normalised"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6PjId7jVg1Cq"
      },
      "outputs": [],
      "source": [
        "def train_clip(model, text_embeddings, image_embeddings, ids, optimizer, loss_fn, batch_size, n_epochs):\n",
        "  assert text_embeddings.shape[0] == image_embeddings.shape[0]\n",
        "\n",
        "  xs_len = text_embeddings.shape[0]\n",
        "  n_batches = int(xs_len/batch_size)\n",
        "  \n",
        "  for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "\n",
        "    shuff_idxs = torch.randperm(xs_len)\n",
        "    for batch in range(n_batches):\n",
        "        batch_idxs = shuff_idxs[batch*batch_size:(batch+1)*batch_size]\n",
        "\n",
        "        batch_text = text_embeddings[batch_idxs]\n",
        "        batch_image = image_embeddings[batch_idxs]\n",
        "        batch_ids = ids[batch_idxs]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(batch_text, batch_image)\n",
        "\n",
        "        target = np.zeros((batch_size, batch_size))\n",
        "        for i in range(batch_size):\n",
        "          target[i] = (batch_ids[i] == batch_ids)\n",
        "        target = torch.Tensor(target.astype(int) * 2 - 1).to(device)\n",
        "\n",
        "        #print(output.shape, target.shape)\n",
        "\n",
        "        loss = loss_fn(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZ8COQJEvgp4"
      },
      "outputs": [],
      "source": [
        "def evaluate_clip(model, test_text, test_image, test_ids):\n",
        "  # zero shot 4 way\n",
        "  shots = np.arange(len(test_ids))\n",
        "  np.random.shuffle(shots)\n",
        "  shots = shots.reshape(len(test_ids) // 4, 4)\n",
        "\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  for i in range(len(shots)):\n",
        "    shot_ids = shots[i]\n",
        "\n",
        "    # Take the description from the first image\n",
        "    shot_text = test_text[shot_ids[0]].unsqueeze(0)\n",
        "    shot_image = test_image[shot_ids]\n",
        "    shot_cat_ids = test_ids[shot_ids]\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "      preds = model(shot_text, shot_image)\n",
        "    \n",
        "    if preds.argmax() == 0:\n",
        "      correct += 1\n",
        "    total += 1\n",
        "  \n",
        "  return correct / total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLaXrEqJgyOP"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tewEKt0rg0Hp"
      },
      "outputs": [],
      "source": [
        "N_CLASSES = 512\n",
        "N_IMAGES_PER_CLASS = 128\n",
        "DIM = 16\n",
        "\n",
        "text_embs, image_embs, ids = generate_fake_embeddings(n_classes=N_CLASSES, n_images_per_class=N_IMAGES_PER_CLASS, dim=DIM)\n",
        "\n",
        "train_text = text_embs[:int(N_CLASSES * N_IMAGES_PER_CLASS * 0.75)]\n",
        "train_image = image_embs[:int(N_CLASSES * N_IMAGES_PER_CLASS * 0.75)]\n",
        "train_ids = ids[:int(N_CLASSES * N_IMAGES_PER_CLASS * 0.75)]\n",
        "\n",
        "test_text = text_embs[int(N_CLASSES * N_IMAGES_PER_CLASS * 0.75):]\n",
        "test_image = image_embs[int(N_CLASSES * N_IMAGES_PER_CLASS * 0.75):]\n",
        "test_ids = ids[int(N_CLASSES * N_IMAGES_PER_CLASS * 0.75):]\n",
        "\n",
        "\n",
        "model = CLIP(train_text.shape[1], train_image.shape[1], 12).to(device)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "loss_fn = nn.MSELoss().to(device)\n",
        "\n",
        "train_clip(model, train_text, train_image, train_ids, optimizer, loss_fn, batch_size=256, n_epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBZIQhk6vqpx"
      },
      "outputs": [],
      "source": [
        "csn = evaluate_clip(model, test_text, test_image, test_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M33dbNwgwIJp",
        "outputId": "8103e948-ff34-4828-803a-513171e09eba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.76904296875"
            ]
          },
          "execution_count": 148,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "csn"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "clip.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
